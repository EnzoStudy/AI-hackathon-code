{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "\n",
    "데이터 분석 및 전처리 코드\n",
    "\n",
    "\n",
    "10월 19일\n",
    "AIDU 대회 예선 코드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파일 업로드 및 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AIDU Library Import\n",
    "from aicentro.session import Session\n",
    "from aicentro.framework.keras import Keras as AiduFrm\n",
    "aidu_session = Session(verify=False)\n",
    "aidu_framework = AiduFrm(session=aidu_session)\n",
    "\n",
    "# 데이터 파일 경로\n",
    "data_path = aidu_framework.config.data_dir\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "# 불필요한 경고 출력을 방지합니다.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#필요한 모델 다운로드\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"xgboost\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"seaborn\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"lightgbm\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"catboost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 명\n",
    "\n",
    "FILE_NAME = 'jaeho_1012_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train file 데이터 디렉토리\n",
    "train_file_dir = data_path + \"/episode1_train_-99.csv\"\n",
    "train_ob_file_dir = data_path + \"/episode1_train_ob.csv\"\n",
    "\n",
    "#Test file 데이터 디렉토리\n",
    "test_file_dir = data_path + \"/episode1_test_-99.csv\"\n",
    "test_ob_file_dir = data_path + \"/episode1_test_ob.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#원본 데이터\n",
    "train_raw_df = pd.read_csv(train_file_dir)\n",
    "train_ob_raw_df = pd.read_csv(train_ob_file_dir)\n",
    "\n",
    "#테스트 원본 데이터 \n",
    "test_raw_df = pd.read_csv(test_file_dir)\n",
    "test_ob_raw_df = pd.read_csv(test_ob_file_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레인 데이터 구조\n",
    "print(train_raw_df.shape)\n",
    "print(train_ob_raw_df.shape)\n",
    "\n",
    "#테스트 데이터셋 구조\n",
    "print(test_raw_df.shape)\n",
    "print(test_ob_raw_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측치 확인\n",
    "print(train_raw_df.isnull().sum().sum())\n",
    "print(train_ob_raw_df.isnull().sum().sum())\n",
    "\n",
    "print(test_raw_df.isnull().sum().sum())\n",
    "print(test_ob_raw_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train ob데이터와 합침 (id를 기준으로)\n",
    "train_raw_df = pd.merge(train_raw_df, train_ob_raw_df, how = 'outer', on='id')\n",
    "train_raw_df = train_raw_df.fillna(0)\n",
    "\n",
    "#test_ob데이터와 합침 (id를 기준으로)\n",
    "test_raw_df = pd.merge(test_raw_df, test_ob_raw_df, how = 'outer', on='id')\n",
    "test_raw_df = test_raw_df.fillna(0)\n",
    "\n",
    "\n",
    "print(train_raw_df.shape)\n",
    "print(test_raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★★★★★★★여기부터 시작 전처리 및 EDA 시작★★★★★★★★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 데이터를 Train 데이터 밑에 합침\n",
    "merge_raw_df = pd.concat([train_raw_df, test_raw_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 후 모델학습을 위한 데이터\n",
    "merge_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col = merge_raw_df.columns\n",
    "all_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 N이상수는 N으로 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n 이상을 n 으로 합치는 컬럼\n",
    "\n",
    "#n이상 숫자를 n로 대치\n",
    "case_oulier_n_col = [\n",
    "                   'voc_cnt_n',\n",
    "                     'accum_usst_day_num_divide_10',\n",
    "                     'tm_cant_call_cnt','tm_etc_cnt',\n",
    "                     'tm_npay_cnt','tm_use_stop_cnt',\n",
    "                     'sm_npay_cnt','sm_thismonth_cnt', 'recnt_icg_date_from_this_year'\n",
    "                     ]\n",
    "\n",
    "\n",
    "def oulier_up_n_num(input_df, col_name, n):\n",
    "    '''\n",
    "    n보다 큰 수는 n으로 처리하는 함수\n",
    "    param:\n",
    "    input_df: 인풋함수:\n",
    "    col_name: 컬럼명\n",
    "    n : n 이상 처리할 숫자\n",
    "    '''\n",
    "    \n",
    "    fun_df = pd.DataFrame()\n",
    "      \n",
    "    def fun(x):\n",
    "        if x > n:\n",
    "            return n\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    fun_df = input_df[col_name].apply(lambda x : fun(x) )\n",
    "    \n",
    "    return fun_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n보다 큰 수 n로 만들기\n",
    "for index in case_oulier_n_col:\n",
    "    merge_df[index] = oulier_up_n_num(merge_raw_df, index, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 0과 0이 아닌것으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n 이상을 n 으로 합치는 컬럼\n",
    "case_alpha_zero_or_not_col = [\n",
    "#                              'voc_inqr_cascnt_n',\n",
    "#                                 'voc_tot_occ_cascnt_n',\n",
    "#                                 'rmonth_qat_chage_voc_cascnt',\n",
    "#                                 'accum_usst_day_num_divide_10',\n",
    "#                                 'tm_cant_call_cnt',\n",
    "#                                 'tm_etc_cnt',\n",
    "#                                 'tm_use_stop_cnt',\n",
    "#                                 'sm_use_stop_cnt',\n",
    "#                                 'sm_use_stop_cnt'\n",
    "                            ]\n",
    "\n",
    "def zero_or_not(input_df, col_name):\n",
    "    '''\n",
    "    0 / 0 아닌수로 대치하여 YN 변경\n",
    "    param:\n",
    "    \n",
    "    input_df: 인풋함수:\n",
    "    col_name: 컬럼명\n",
    "    '''\n",
    "    \n",
    "    fun_df = pd.DataFrame()\n",
    "    \n",
    "    def fun(x):\n",
    "        if x ==0:\n",
    "            return 'N'\n",
    "        else:\n",
    "            return \"Y\"\n",
    "        \n",
    "    fun_df = input_df[col_name].apply(lambda x : fun(x) )\n",
    "    \n",
    "    return fun_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n보다 큰 수 n로 만들기\n",
    "for index in case_alpha_zero_or_not_col:\n",
    "    merge_df[index] = zero_or_not(merge_raw_df, index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 이상치를 IQR highest로 교체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    '''\n",
    "    outlier 추출하여 index로 리턴\n",
    "    \n",
    "    param \n",
    "    df: 추출하고자 하는 데이터 프레임 전체\n",
    "    column : 이상치 확인 하려는 컬럼\n",
    "    weight : 가중치\n",
    "    \n",
    "    '''\n",
    "  # target 값과 상관관계가 높은 열을 우선적으로 진행\n",
    "    quantile_25 = np.percentile(df[column].values, 25)\n",
    "    quantile_75 = np.percentile(df[column].values, 75)\n",
    "\n",
    "    IQR = quantile_75 - quantile_25\n",
    "    IQR_weight = IQR*weight\n",
    "  \n",
    "    lowest = quantile_25 - IQR_weight\n",
    "    highest = quantile_75 + IQR_weight\n",
    "    \n",
    "    iqr = highest\n",
    "\n",
    "    outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index\n",
    "\n",
    "    return iqr, outlier_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_up_iqr_num(input_df, col_name, iqr):\n",
    "    '''\n",
    "    iqr보다 큰 수는 iqr로 처리하는 함수\n",
    "    param:\n",
    "    input_df : 인풋함수\n",
    "    col_name : 컬럼명\n",
    "    n : n 이상 처리할 숫자\n",
    "    '''\n",
    "    fun_df = pd.DataFrame()\n",
    "    \n",
    "    def fun(x):\n",
    "        if x > iqr:\n",
    "            return iqr\n",
    "        else : \n",
    "            return x\n",
    "        \n",
    "    fun_df = input_df[col_name].apply(lambda x : fun(x))\n",
    "    \n",
    "    return fun_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_iqr_col =[\n",
    "  'r3m_avg_bill_amt', 'r3m_mphon_avg_arpu_amt', 'r3m_iptv_avg_arpu_amt', \n",
    "       'r3m_inet_avg_arpu_amt','r6m_mphon_avg_arpu_amt', 'r6m_iptv_avg_arpu_amt',\n",
    "       'r6m_inet_avg_arpu_amt',\n",
    "              ]\n",
    "\n",
    "#이상치 값을 IQR highest로 대치하여 정상화\n",
    "for index in case_iqr_col:\n",
    "    # 함수 사용해서 이상치 값 삭제\n",
    "    iqr, oulier_idx = get_outlier(df=merge_raw_df, column=index, weight=2.3)\n",
    "    merge_df[index] = outlier_up_iqr_num(merge_raw_df, index, iqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라벨 인코딩할 컬럼\n",
    "case_label_encoding_col=[\n",
    "                        'label_payment_yn',\n",
    "                        'inet_sbsc_yn',\n",
    "                        'soip_sbsc_yn',\n",
    "                        'iptv_sbsc_yn',\n",
    "                        'pstn_sbsc_yn',\n",
    "                        'pstn_sbsc_yn',\n",
    "                        'dt_stop_yn',\n",
    "                        'mphon_comb_yn',\n",
    "                        'mphon_sbsc_yn',\n",
    "                        'inet_comb_yn',\n",
    "                        'iptv_comb_yn',\n",
    "                        'npay_yn',\n",
    "                        'hndset_rmnd_insl_mons_yn',\n",
    "                        'r6m_inet_pssn_comb_yn',\n",
    "                        'r6m_iptv_pssn_comb_yn',\n",
    "                        'kids_wrlin_adtn_svc_sbsc_yn',\n",
    "                        'r6m_iptv_first_ppv_use_yn',\n",
    "                        'bill_rmny_npay_tms_itg_cd',\n",
    "                        'r3m_avg_bill_amt_100000_over_yn',\n",
    "                        'r3m_wless_data_use_qnt_100000_over_yn',\n",
    "                        'insur_prod_sbsc_yn',\n",
    "    \n",
    "\n",
    "                        ]\n",
    "\n",
    "\n",
    "def labelencoding( input_df, col):\n",
    "    from sklearn import preprocessing\n",
    "    le=preprocessing.LabelEncoder()\n",
    "    \n",
    "    #라벨링 전처리 후 데이터\n",
    "    le_eda_df = pd.DataFrame()\n",
    "    \n",
    "        \n",
    "    #전처리하기 전에 해당 컬럼 전처리 후 컬럼에 추가\n",
    "    le_eda_df[col] = input_df[col]\n",
    "\n",
    "    if col =='label_payment_yn':\n",
    "        le_eda_df = le_eda_df.fillna('N')\n",
    "        \n",
    "    # 라벨 인코딩\n",
    "    le.fit(le_eda_df[col])\n",
    "\n",
    "    # train-set, test-set 둘다 Transform\n",
    "    le_eda_df[col] = le.transform(le_eda_df[col])\n",
    "    \n",
    "    return le_eda_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩 \n",
    "for index in case_label_encoding_col:\n",
    "    merge_df[index] = labelencoding(merge_raw_df, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_onehot_encoding_col = [\n",
    "   'pay_way_nm',\n",
    " 'acrnd_dsp_meth_nm',\n",
    " 'anals_2_prod_level_nm',\n",
    " 'bprod_lctg_nm',\n",
    " 'wless_terml_gun_div_nm',\n",
    " 'wless_terml_gun_div_type'\n",
    "                            ]\n",
    "\n",
    "def onehot_encoding(input_df, col):\n",
    "    \n",
    "    #카테고리 갯수로 데이터프레임 만듬\n",
    "    index_value_df =pd.DataFrame ()\n",
    "    index_value_df['cat'] = list(input_df[col].value_counts().index)                           \n",
    "    index_value_df['value']= list(input_df[col].value_counts())\n",
    "    \n",
    "    fun_df = pd.DataFrame()\n",
    "    \n",
    "    fun_df = pd.get_dummies(input_df[col], prefix=col)\n",
    "    \n",
    "    return fun_df\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in case_onehot_encoding_col:\n",
    "    \n",
    "    onehot_df =onehot_encoding(merge_raw_df,index )\n",
    "       \n",
    "    merge_df[onehot_df.columns] = onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_onehot_encoding_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 전처리 안 해준 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 컬럼에서, 전처리했던 컬럼 제거하여 전처리 안한 컬럼 추출\n",
    "add_raw_col=list(set(merge_raw_df.columns)-set(merge_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 안한 컬럼 추가\n",
    "for index in add_raw_col:\n",
    "    merge_df[add_raw_col] = merge_raw_df[add_raw_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_raw_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in merge_df.columns:\n",
    "    if merge_df[index].dtype =='O':\n",
    "        merge_df[index] = labelencoding(merge_df,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#정규화 할 컬럼 , 나머지는 roboster 정규화 진행\n",
    "nomalize_col=[ 'svc_use_mons_num_divide_10','sm_athrt_trmn_cnt', 'sm_term_stop_cnt', 'tm_cant_collect_cnt','efct_iptv_sbsc_cascnt', 'hndset_rmnd_insl_mons_num',   'npay_tms_cnt',    'efct_pstn_sbsc_cascnt']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "#로보스터 할 데이터\n",
    "roboster_col = list(set(merge_df.columns)-set(nomalize_col))\n",
    "merge_df[roboster_col] = scaler.fit_transform( merge_df[roboster_col])\n",
    "\n",
    "\n",
    "#정규화 할 데이터\n",
    "scaler = MinMaxScaler()\n",
    "#nomalize_col = merge_df.columns\n",
    "merge_df[nomalize_col] = scaler.fit_transform( merge_df[nomalize_col])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 변수 중요도 낮은 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = [ 'id','voc_sbsc_inqr_cascnt_n',\n",
    "            'voc_dscnt_cascnt_n','voc_icnv_cascnt_n',\n",
    "            'tm_athrt_trmn_cnt','tm_handling_complaint_cnt',\n",
    "            'tm_promise_pay_cnt', 'tm_term_stop_cnt']\n",
    "\n",
    "# drop_list=['sm_term_stop_cnt', 'voc_icnv_cascnt_n', 'tm_cant_collect_cnt']\n",
    "\n",
    "merge_df.drop( drop_col,axis=1,inplace=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 모델 전처리 출력\n",
    "\n",
    "어떤 전처리를 했는지 출력해주는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = data_path + \"/episdode1_train_description.csv\"\n",
    "description_ob = data_path + \"/episdode1_train_ob_description.csv\"\n",
    "\n",
    "description_df=pd.read_csv(description)\n",
    "description_ob_df=pd.read_csv(description_ob)\n",
    "\n",
    "description_df = pd.concat([description_df,description_ob_df], ignore_index=True)\n",
    "\n",
    "\n",
    "eda_column_df = pd.DataFrame()\n",
    "eda_column_df['기준컬럼'] = merge_raw_df.columns\n",
    "\n",
    "\n",
    "for row in range(eda_column_df.shape[0]):\n",
    "    str_eng= eda_column_df.loc[row,'기준컬럼']\n",
    "    for i in description_df['컬럼 영문명']:\n",
    "        if i in str_eng:\n",
    "            eda_column_df.loc[row,'컬럼 한글명'] = description_df.loc[description_df['컬럼 영문명']==i, '컬럼 한글명'].values[0]\n",
    "\n",
    "# 전체 - 으로 세팅            \n",
    "eda_column_df['전처리결과'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eda_check (eda_fun_col,check_name):\n",
    "\n",
    "    for i in eda_fun_col:\n",
    "        try:\n",
    "            for row in range(eda_column_df.shape[0]):\n",
    "                str_name = eda_column_df.loc[row,'기준컬럼']\n",
    "                if str_name in i:\n",
    "                    row_name = str_name\n",
    "            data_st =eda_column_df.loc[eda_column_df['기준컬럼']==row_name,'전처리결과'].values[0]\n",
    "\n",
    "            if data_st == '-':\n",
    "                eda_column_df.loc[eda_column_df['기준컬럼']==row_name,'전처리결과']= check_name\n",
    "            else:\n",
    "                if check_name not in data_st:\n",
    "                    eda_column_df.loc[eda_column_df['기준컬럼']==row_name,'전처리결과']= data_st+\"/ \" +check_name\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_check(drop_col,'삭제' )\n",
    "eda_check(case_oulier_n_col,'nton' )\n",
    "eda_check(case_alpha_zero_or_not_col,'0roNot' )\n",
    "eda_check(case_iqr_col,'IQR' )\n",
    "eda_check(case_onehot_encoding_col,'원핫인코딩' )\n",
    "eda_check(add_raw_col,'미처리' )\n",
    "eda_check(nomalize_col,'정규화' )\n",
    "eda_check(roboster_col,'로보스터' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "eda_column_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 한 결과 출력해주는 부분 \n",
    "\n",
    "eda_file_name = data_path + \"/fit/eda_result_check_\"+FILE_NAME+\".csv\"\n",
    "eda_column_df.to_csv(eda_file_name, index=False )\n",
    "print(eda_file_name,\"파일이 출력되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 상관값 높은 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = merge_df.iloc[:train_raw_df.shape[0]]\n",
    "test_df = merge_df.iloc[train_raw_df.shape[0]:]\n",
    "\n",
    "#상관관계 비교를 위해 \n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_df.drop(['label_payment_yn'],axis=1), train_df['label_payment_yn'],random_state=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#상관관계 비교하여 드롭할 컬럼 추출\n",
    "print(x_train.shape)\n",
    "corr_matrix = x_train.corr()\n",
    "theshold =0.95\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "#상관값 높은 컬럼\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) >theshold)]\n",
    "\n",
    "merge_df.drop(to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 PCA 차원 축소\n",
    "여긴 이어지지는 않고 필요하면 원하는 컬럼만 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "compoent = int(merge_df.shape[1]*0.8)\n",
    "\n",
    "pca = PCA(n_components=compoent) # 주성분을 몇개로 할지 결정\n",
    "\n",
    "\n",
    "printcipalComponents = pca.fit_transform(merge_df.drop(['label_payment_yn'],axis=1))\n",
    "\n",
    "principalDf = pd.DataFrame(data=printcipalComponents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 분리 및 모델 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = merge_df.iloc[:train_raw_df.shape[0]]\n",
    "test_df = merge_df.iloc[train_raw_df.shape[0]:]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 완료한 파일로 출력하여 모델학습으로 이어짐\n",
    "\n",
    "test_file_name =data_path + \"/pre_data/test_preprocessing_\"+FILE_NAME+\".csv\"\n",
    "    \n",
    "train_file_name = data_path + \"/pre_data/train_preprocessing_\"+FILE_NAME+\".csv\"\n",
    "    \n",
    "# 학습할 부분과 테스트 할 부분 나눔\n",
    "test_df.drop(['label_payment_yn'],axis=1, inplace=True)\n",
    "test_df.to_csv(test_file_name, index=False)\n",
    "print(test_file_name,\": 파일이 출력되었습니다.\")\n",
    "\n",
    "train_df.to_csv(train_file_name, index=False)\n",
    "print(train_file_name, \": 파일이 출력되었습니다.\")    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
