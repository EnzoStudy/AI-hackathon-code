{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "\n",
    "모델 학습을 위한 코드\n",
    "\n",
    "- 모델별 가중치 줘서 계산\n",
    "- 모델 선택 가능\n",
    "\n",
    "10월 06일 23시\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파일 업로드 및 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AIDU Library Import\n",
    "from aicentro.session import Session\n",
    "from aicentro.framework.keras import Keras as AiduFrm\n",
    "aidu_session = Session(verify=False)\n",
    "aidu_framework = AiduFrm(session=aidu_session)\n",
    "\n",
    "# 데이터 파일 경로\n",
    "data_path = aidu_framework.config.data_dir\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "# 불필요한 경고 출력을 방지합니다.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#필요한 모델 다운로드\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"xgboost\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"seaborn\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"lightgbm\"])\n",
    "subprocess.call([sys.executable,\"-m\",\"pip\",\"install\",\"catboost\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 러닝 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#러닝을 위한 모델 선택\n",
    "model_select = {\n",
    "    \n",
    "    'randomforest' : 0,\n",
    "    'lightgbm' : 1,\n",
    "    'catboost' : 1,\n",
    "    'xgboost' : 0,\n",
    "    'softvoting' : 0  \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 출력 디렉토리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 h5 파일 저장 디렉토리\n",
    "randomforest_model_dir_ = data_path + \"/fit/model/randomforest_model.h5\"\n",
    "lightgbm_model_dir = data_path + \"/fit/model/lightgbm_model.h5\"\n",
    "catboost_model_dir = data_path + \"/fit/model/catboost_model.h5\"\n",
    "xgboost_model_dir = data_path + \"/fit/model/xgb_model.h5\"\n",
    "soft_voting_dir = data_path + \"/fit/model/soft_voting.h5\"\n",
    "\n",
    "#acc 결과 저장 디렉토리\n",
    "acc_data_dir = data_path + \"/fit/accuracy/acc_data.csv\"\n",
    "\n",
    "#특징 중요도 확인 csv\n",
    "feature_importance_dir = data_path + \"/fit/feature_importance/model_Feature_Importance.csv\"\n",
    "\n",
    "# 최종 결과 파일명\n",
    "result_file_name = data_path +'/제출데이터/2차제출_1007_0100.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 기본 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Train file 데이터 디렉토리\n",
    "train_file_dir = data_path + \"/episode1_train.csv\"\n",
    "train_ob_file_dir = data_path + \"/episode1_train_ob.csv\"\n",
    "\n",
    "#Test file 데이터 디렉토리\n",
    "test_file_dir = data_path + \"/episode1_test.csv\"\n",
    "test_ob_file_dir = data_path + \"/episode1_test_ob.csv\"\n",
    "\n",
    "\n",
    "#전처리 후 test 파일명\n",
    "test_file_name =data_path + \"/pre_data/test_preprocessing.csv\"\n",
    "\n",
    "#전처리 후 train 파일명\n",
    "train_file_name = data_path + \"/pre_data/train_preprocessing.csv\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_split(input_split_df, col_name ='include_ob' ):\n",
    "    '''\n",
    "    ob 데이터가 있는 것과 ,아닌것으로 나누는 함수\n",
    "    \n",
    "    param \n",
    "    input_split_df : 분리하고 싶은 데이터\n",
    "    col_name : 분리를 하기위한 구분 컬럼\n",
    "    '''\n",
    "    \n",
    "    #데이터 2개 선언\n",
    "    include_ob_df = pd.DataFrame()\n",
    "    include_not_ob_df = pd.DataFrame()\n",
    "    \n",
    "    #구분 컬럼이 1 / 0 에 따라 판단하여 데이터 분리\n",
    "    include_ob_df = input_split_df[input_split_df[col_name]==1.0]\n",
    "    include_not_ob_df = input_split_df[input_split_df[col_name]==0.0]\n",
    "    \n",
    "    \n",
    "    include_ob_df.drop(['include_ob'], axis=1, inplace =True)\n",
    "    include_not_ob_df.drop(['tm_athrt_trmn_cnt', 'tm_cant_call_cnt', \n",
    "       'tm_claim_cnt', 'tm_etc_cnt', 'tm_handling_complaint_cnt',\n",
    "       'tm_npay_cnt', 'tm_promise_pay_cnt', 'tm_term_stop_cnt',\n",
    "       'tm_use_stop_cnt', 'sm_athrt_trmn_cnt', 'sm_npay_cnt',\n",
    "        'sm_thismonth_cnt', 'sm_use_stop_cnt',\n",
    "       'include_ob'],axis=1, inplace=True)\n",
    "    \n",
    "    return include_ob_df, include_not_ob_df\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ob 데이터가 있는것과 아닌것으로 분류\n",
    "train_ob_df , train_not_ob_df =model_split(train_df,'include_ob')\n",
    "test_ob_df , test_not_ob_df =model_split(test_df,'include_ob')\n",
    "\n",
    "#뒤에 결과값 출력을 위해 id와 include_ob 저장\n",
    "temp_df = pd.read_csv(test_file_dir)\n",
    "test_id_df = pd.DataFrame()\n",
    "test_id_df['id'] = temp_df['id']\n",
    "test_id_df['include_ob'] = test_df['include_ob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_ob_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#데이터도 include_ob 컬럼 지움\n",
    "train_df.drop(['include_ob'],axis=1, inplace=True)\n",
    "test_df.drop(['include_ob'],axis=1, inplace=True)\n",
    "\n",
    "print(train_ob_df.shape, train_not_ob_df.shape)\n",
    "print(test_ob_df.shape, test_not_ob_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습하기위한 train과 validation 나눔\n",
    "# 일반 데이터\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_df.drop(['label_payment_yn'],axis=1), train_df['label_payment_yn'],random_state=21)\n",
    "\n",
    "# ob데이터가 있는것고 없는것으로 나눈것\n",
    "x_train_ob, x_valid_ob, y_train_ob, y_valid_ob = train_test_split(train_ob_df.drop(['label_payment_yn'],axis=1), train_ob_df['label_payment_yn'],random_state=21)\n",
    "x_train_not_ob, x_valid_not_ob, y_train_not_ob, y_valid_not_ob = train_test_split(train_not_ob_df.drop(['label_payment_yn'],axis=1), train_not_ob_df['label_payment_yn'],random_state=21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 후 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_predictions = {}\n",
    "\n",
    "colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',\n",
    "          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',\n",
    "          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', \n",
    "          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',\n",
    "          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', \n",
    "          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'\n",
    "         ]\n",
    "\n",
    "\n",
    "#acc를 구해서 시각화해줌\n",
    "def acc_eval(name_, pred, actual):\n",
    "    global predictions\n",
    "    global colors\n",
    "\n",
    "    acc = (pred==actual).mean()\n",
    "    my_predictions[name_] = acc\n",
    "\n",
    "\n",
    "    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=False)  # 정확도 내림차순으로 sort\n",
    "    \n",
    "    df = pd.DataFrame(y_value, columns=['model', 'acc'])\n",
    "    print(df)\n",
    "    min_ = df['acc'].min() -1\n",
    "    max_ = 1.2\n",
    "    \n",
    "    length = len(df)\n",
    "    \n",
    "    plt.figure(figsize=(10, length))\n",
    "    ax = plt.subplot()\n",
    "    ax.set_yticks(np.arange(len(df)))\n",
    "    ax.set_yticklabels(df['model'], fontsize=15)\n",
    "    bars = ax.barh(np.arange(len(df)), df['acc'])\n",
    "    \n",
    "    for i, v in enumerate(df['acc']):\n",
    "        idx = np.random.choice(len(colors))\n",
    "        bars[i].set_color(colors[idx])\n",
    "        ax.text(v+0.1, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')\n",
    "        \n",
    "    plt.title('Accuracy', fontsize=18)\n",
    "    plt.xlim(min_,max_)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#acc를 구해서 시각화해줌(cross_val_score 사용하는 경우)\n",
    "def acc_eval_add(name_, acc):\n",
    "    global predictions\n",
    "    global colors\n",
    "\n",
    "    my_predictions[name_] = acc\n",
    "\n",
    "\n",
    "    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=False)  # 정확도 내림차순으로 sort\n",
    "    \n",
    "    df = pd.DataFrame(y_value, columns=['model', 'acc'])\n",
    "    print(df)\n",
    "    min_ = df['acc'].min() -1\n",
    "    max_ = 1.2\n",
    "    \n",
    "    length = len(df)\n",
    "    \n",
    "    plt.figure(figsize=(10, length))\n",
    "    ax = plt.subplot()\n",
    "    ax.set_yticks(np.arange(len(df)))\n",
    "    ax.set_yticklabels(df['model'], fontsize=15)\n",
    "    bars = ax.barh(np.arange(len(df)), df['acc'])\n",
    "    \n",
    "    for i, v in enumerate(df['acc']):\n",
    "        idx = np.random.choice(len(colors))\n",
    "        bars[i].set_color(colors[idx])\n",
    "        ax.text(v+0.1, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')\n",
    "        \n",
    "    plt.title('Accuracy', fontsize=18)\n",
    "    plt.xlim(min_,max_)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    df.to_csv(acc_data_dir,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델학습 (랜덤 포레스트, XGboost, Light GBM, CATboost )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 랜덤포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def randomforest_model_fit (x_train_random, x_valid_random, y_train_random, y_valid_random, model_name='RF'):\n",
    "    '''\n",
    "    랜덤포레스트 모델 학습을 위한 함수\n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    \n",
    "    #모델선언\n",
    "    model_rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "    score = cross_val_score(model_rf, x_train_random, y_train_random).mean()\n",
    "    cross_val_score(model_rf, x_valid_random, y_valid_random).mean()\n",
    "    \n",
    "    #학습\n",
    "    model_rf.fit(x_train_random, y_train_random)\n",
    "    \n",
    "    #예측 및 결과 확인\n",
    "    pred=model_rf.predict(x_valid_random)\n",
    "    acc_eval_add(model_name,score) #기본\n",
    "    \n",
    "    #모델 및 예측값 return\n",
    "    return model_rf, pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#랜덤포레스트 모델 러닝 선택시\n",
    "   \n",
    "if model_select['randomforest'] == 1:\n",
    "    randomforest_model, randomforest_pred = randomforest_model_fit (x_train, x_valid, y_train, y_valid, 'randomforest' )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def xgboost_model_fit (x_train_xg, x_valid_xg, y_train_xg, y_valid_xg, model_name='XGB'):\n",
    "    '''\n",
    "    Cat boost 모델 학습을 위한 함수\n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    \n",
    "     #모델 선언\n",
    "    xgb_model = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "    start=time.time()\n",
    "\n",
    "    eval_set = [(x_valid_xg, y_valid_xg)]\n",
    "    \n",
    "    #학습\n",
    "    #logloss가 10회 계선되지 않으면 중단하도록 early stopping 설정 n_estimators=100에서는 얼리스탑하지 않음\n",
    "    xgb_model.fit(x_train_xg, y_train_xg, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=2)\n",
    "\n",
    "    (time.time()-start)/60\n",
    "    \n",
    "    pred= xgb_model.predict(x_valid_xg)\n",
    "    \n",
    "    acc_eval(model_name, pred,y_valid) #기본\n",
    "    \n",
    "    #모델 및 예측값 return\n",
    "    return xgb_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost 모델 선택시\n",
    "\n",
    "if model_select['xgboost'] == 1:\n",
    "    #모델 학습\n",
    "    xgboost_model, xgboost_pred = xgboost_model_fit (x_train, x_valid, y_train, y_valid, 'xgboost' )\n",
    "    #모델 save\n",
    "    xgboost_model.save_model(xgboost_model_dir )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def catboost_model_fit (x_train_cat, x_valid_cat, y_train_cat, y_valid_cat, model_name='CAT'):\n",
    "    '''\n",
    "    Cat boost 모델 학습을 위한 함수\n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    #모델 선언\n",
    "    cat_model= CatBoostClassifier()\n",
    "    start=time.time()\n",
    "    \n",
    "    #학습\n",
    "    cat_model.fit(x_train_cat, y_train_cat, verbose=2 )\n",
    "    \n",
    "    #예측 및 결과 확인\n",
    "    pred = cat_model.predict(x_valid_cat)\n",
    "    acc_eval(model_name ,pred, y_valid_cat)\n",
    "    \n",
    "    #모델 및 예측값 return\n",
    "    return cat_model, pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_model_kfold_fit (x_train_cat, x_valid_cat, y_train_cat, y_valid_cat, model_name='CAT'):\n",
    "    '''\n",
    "    Cat boost 모델 학습을 위한 함수 , KFOLD 포함\n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    \n",
    "    #모델 선언\n",
    "    cat_model= CatBoostClassifier()\n",
    "    start=time.time()\n",
    "    \n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state = 71)\n",
    "    for tr_idx, va_idx in kf.split(x_train):\n",
    "        x_train_fold, x_valid_fold = x_train_cat.iloc[tr_idx], x_train_cat.iloc[va_idx]\n",
    "        y_train_fold, y_valid_fold = y_train_cat.iloc[tr_idx], y_train_cat.iloc[va_idx]\n",
    "\n",
    "        cat_model.fit(x_train_fold, y_train_fold, verbose=0 )\n",
    "\n",
    "    #예측 및 결과 확인\n",
    "    pred = cat_model.predict(x_valid_cat)\n",
    "    acc_eval(model_name ,pred, y_valid_cat)\n",
    "    \n",
    "    #모델 및 예측값 return\n",
    "    return cat_model, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_select['catboost'] == 1:\n",
    "    #모델 학습 진행\n",
    "#     cat_ob_model,cat_ob_pred = cat_boost_model (x_train_ob, x_valid_ob, y_train_ob, y_valid_ob, 'CAT_OB' )\n",
    "#     cat_not_ob_model,cat_not_ob_pred = cat_boost_model (x_train_not_ob, x_valid_not_ob, y_train_not_ob, y_valid_not_ob, 'CAT_NOT_OB' )\n",
    "    \n",
    "    catboost_model, catboost_pred = catboost_model_fit (x_train, x_valid, y_train, y_valid, 'catboost' )\n",
    "    catboost_model.save_model(catboost_model_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_model_fit(x_train_lgbm, x_valid_lgbm, y_train_lgbm, y_valid_lgbm, model_name='LGBM'):\n",
    "\n",
    "    '''\n",
    "    lgbm 모델 학습을 위한 함수 \n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    \n",
    "    lgbm_model = LGBMClassifier()\n",
    "\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "    eval_set = [(x_valid_lgbm, y_valid_lgbm)]\n",
    "\n",
    "    #logloss가 10회 계선되지 않으면 중단하도록 early stopping 설정 n_estimators=100에서는 얼리스탑하지 않음\n",
    "    lgbm_model.fit(x_train_lgbm, y_train_lgbm, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=2)\n",
    "\n",
    "    (time.time()-start)/60\n",
    "\n",
    "    pred = lgbm_model.predict(x_valid_lgbm)\n",
    "    \n",
    "    acc_eval(model_name, pred,y_valid_lgbm) #기본\n",
    "    \n",
    "    return lgbm_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold 로 교차 검증 실시\n",
    "\n",
    "def lightgbm_model_kfold_fit (x_train_lgbm, x_valid_lgbm, y_train_lgbm, y_valid_lgbm, model_name='LGBM'):\n",
    "\n",
    "    lgbm_model = LGBMClassifier()\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state = 71)\n",
    "    for tr_idx, va_idx in kf.split(x_train):\n",
    "        x_train_fold, x_valid_fold = x_train_lgbm.iloc[tr_idx], x_train_lgbm.iloc[va_idx]\n",
    "        y_train_fold, y_valid_fold = y_train_lgbm.iloc[tr_idx], y_train_lgbm.iloc[va_idx]\n",
    "\n",
    "        start=time.time()\n",
    "\n",
    "        eval_set = [(x_valid_fold, y_valid_fold)]\n",
    "\n",
    "        #logloss가 10회 계선되지 않으면 중단하도록 early stopping 설정 n_estimators=100에서는 얼리스탑하지 않음\n",
    "        lgbm_model.fit(x_train_fold, y_train_fold, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=2)\n",
    "\n",
    "        (time.time()-start)/60\n",
    "        \n",
    "    \n",
    "    pred = lgbm_model.predict(x_valid_lgbm)\n",
    "    \n",
    "    acc_eval(model_name, pred, y_valid_lgbm) #기본\n",
    "    \n",
    "    return lgbm_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 시작\n",
    "if model_select['lightgbm'] ==1 :\n",
    "    \n",
    "    #일반 모델 \n",
    "    lightgbm_model, lightgbm_pred = lightgbm_model_fit (x_train, x_valid, y_train, y_valid, 'lightgbm' )\n",
    "    lightgbm_model.booster_.save_model(lightgbm_model_dir )\n",
    "    \n",
    "    #OB 분리 모델\n",
    "#     lgbm_ob_model ,lgbm_ob_pred = lgbm_model(x_train_ob, x_valid_ob, y_train_ob, y_valid_ob, 'LGBM_OB')\n",
    "#     lgbm_not_ob_model ,lgbm_not_ob_pred = lgbm_model(x_train_not_ob, x_valid_not_ob, y_train_not_ob, y_valid_not_ob , 'LGBM_not_OB')\n",
    "\n",
    "#     lgbm_ob_model.booster_.save_model(data_path + \"/fit/LGBM_ob_model.h5\" )\n",
    "#     lgbm_not_ob_model.booster_.save_model(data_path + \"/fit/LGBM_not_ob_model.h5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 선택 및 앙상블\n",
    "## softvoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifier\n",
    "# VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 라이브러리 import\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softvoting_model_fit (x_train_soft, x_valid_soft, y_train_soft, y_valid_soft, model_name='softvote'):\n",
    "    '''\n",
    "    softvote 모델 학습을 위한 함수 , KFOLD 포함\n",
    "    \n",
    "    param\n",
    "    x_train_cat: x train data\n",
    "    x_valid_cat: x valid data\n",
    "    y_train_cat: y train data\n",
    "    y_valid_cat: y valid data\n",
    "    model_name: 모델 학습 후 그래프 출력을 위한 텍스트\n",
    "    '''\n",
    "    \n",
    "    # ensemble 할 model 정의\n",
    "    models = [\n",
    "        #('rfc', RandomForestClassifier()),\n",
    "        ('xgb', XGBClassifier()),\n",
    "        ('lgbm', LGBMClassifier()),\n",
    "        ('dtc', DecisionTreeClassifier()),\n",
    "\n",
    "    ]\n",
    "\n",
    "    # soft vote\n",
    "    soft_vote  = VotingClassifier(models, voting='soft')\n",
    "    #soft_vote_cv = cross_validate(soft_vote, x_train_soft, y_train_soft, cv=2)\n",
    "    soft_vote.fit(x_train_soft, y_train_soft)\n",
    "    pred = soft_vote.predict(x_valid_soft)\n",
    "    \n",
    "    acc_eval(model_name, pred, y_valid_soft) #기본\n",
    "\n",
    "    # print\n",
    "    print(\"Soft_Vote Test Accuracy: \", accuracy_score(y_valid_soft, pred))\n",
    "    print(classification_report(y_valid, pred, target_names=['N','Y']))\n",
    "    print(confusion_matrix(y_true=y_valid, y_pred=pred))\n",
    "\n",
    "    return soft_vote, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델학습 \n",
    "if model_select['softvoting'] == 1:\n",
    "    softvoting_model, soft_pred = softvoting_model_fit(x_train, x_valid, y_train, y_valid, 'softvoting' )\n",
    "    softvoting_model.save_model(soft_voting_model_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수 중요도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    변수 중요도를 출력\n",
    "    frature_importance_model 에 학습한 모델 하나 선택\n",
    "    'randomforest' , 'xgboost', 'catboost',  'lightgbm', 'softvoting'    \n",
    "'''\n",
    "\n",
    "feature_importance_model = \"catboost\"\n",
    "\n",
    "#모델 실수로 선택 안했거나 잘못 골랐을때 하나 골라서 출력\n",
    "if (feature_importance_model ==\" \")  or  (model_select[feature_importance_model] != 1):\n",
    "    for col in model_select:\n",
    "        if model_select[col] == 1 :\n",
    "            feature_importance_model = col\n",
    "           \n",
    "# 해당 모델 가져오기\n",
    "if feature_importance_model == 'randomforest':\n",
    "    fit_model = randomforest_model\n",
    "elif feature_importance_model == 'lightgbm':\n",
    "    fit_model = lightgbm_model\n",
    "elif feature_importance_model == 'catboost':\n",
    "    fit_model = catboost_model\n",
    "elif feature_importance_model == 'xgboost':\n",
    "    fit_model = xgboost_model\n",
    "elif feature_importance_model == 'softvoting':\n",
    "    fit_model = soft_pred\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "#컬럼명과 중요도 데이터 가져옴\n",
    "feature_importance_df['col'] = x_train.columns\n",
    "feature_importance_df['val'] = fit_model.feature_importances_\n",
    "\n",
    "#중요 순서로 sort\n",
    "feature_importance_df.sort_values(by='val',axis=0,ascending=False,inplace=True)\n",
    "\n",
    "#그래프 출력\n",
    "plt.figure(figsize=(25, 5))\n",
    "plt.bar(feature_importance_df['col'],feature_importance_df['val'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#파일 출력\n",
    "feature_importance_df.to_csv( feature_importance_dir )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 여러개 합쳐서 결과 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 가중치\n",
    "model_weight = {\n",
    "    \n",
    "    'randomforest' : 0.1,\n",
    "    'xgboost' :0.20,\n",
    "    'catboost' :0.30,\n",
    "    'lightgbm' : 0.4,\n",
    "    'softvoting' : 0  \n",
    "    \n",
    "}\n",
    "\n",
    "#모델 학습 선택 출력\n",
    "model_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합친 모델로 valid accuracy 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_accuracy_df = pd.DataFrame()\n",
    "\n",
    "valid_accuracy_df['result']= (list(np.zeros(y_valid.shape[0])))\n",
    "\n",
    "for index in model_weight:\n",
    "    \n",
    "    #모델을 학습 했으면\n",
    "    if model_select[index] == 1:\n",
    "        \n",
    "        # 해당 모델 가져오기\n",
    "        if index == 'randomforest':\n",
    "            index_pred = randomforest_pred\n",
    "        elif index == 'lightgbm':\n",
    "            index_pred = lightgbm_pred\n",
    "        elif index == 'catboost':\n",
    "            index_pred = catboost_pred\n",
    "        elif index == 'xgboost':\n",
    "            index_pred = xgboost_pred\n",
    "        elif index == 'softvoting':\n",
    "            index_pred = soft_pred\n",
    "        \n",
    "        #각 모델별로 가중치 만듬 반영하여 계산\n",
    "        valid_accuracy_df['result'] =  valid_accuracy_df['result'] +pd.Series(index_pred)*model_weight[index]\n",
    " \n",
    "valid_accuracy_df\n",
    "valid_accuracy_df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치로 계산한 값을 확인\n",
    "valid_accuracy_df.loc[valid_accuracy_df['result']>=0.5, 'result'] = 1\n",
    "valid_accuracy_df.loc[valid_accuracy_df['result']<0.5, 'result'] = 0\n",
    "\n",
    "valid_accuracy_df['result'].astype('int')\n",
    "\n",
    "\n",
    "\n",
    "acc_eval('model_weight_cal', valid_accuracy_df['result'].values, y_valid ) #기본\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_result_df = pd.DataFrame()\n",
    "model_result_df['result'] = (list(np.zeros(test_df.shape[0])))\n",
    "\n",
    "#학습한 모델만 각각 예측 결과 확인하여 데이터 프레임 만듬\n",
    "for index in model_select:\n",
    "    \n",
    "    #모델을 학습 했으면\n",
    "    if model_select[index] == 1:\n",
    "        \n",
    "        # 해당 모델 가져오기\n",
    "        if index == 'randomforest':\n",
    "            index_model = randomforest_model\n",
    "        elif index == 'lightgbm':\n",
    "            index_model = lightgbm_model\n",
    "        elif index == 'catboost':\n",
    "            index_model = catboost_model\n",
    "        elif index == 'xgboost':\n",
    "            index_model = xgboost_model\n",
    "        elif index == 'softvoting':\n",
    "            index_model = softvoting_model\n",
    "        \n",
    "        #모델 예측\n",
    "        index_pred = index_model.predict(test_df)\n",
    "        \n",
    "        \n",
    "        #예측값 데이터프레임에 추가\n",
    "        model_result_df[index] = pd.Series(index_pred)\n",
    "        \n",
    "        \n",
    "        #각 모델별로 가중치 만듬 반영하여 계산\n",
    "        model_result_df['result'] =  model_result_df['result'] +pd.Series(index_pred)*model_weight[index]\n",
    "        \n",
    "        \n",
    "        #결과 출력\n",
    "        print(index ,\"예측 완료\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_df\n",
    "model_result_df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#계산한 weight를 기준으로 0.5보다 크면 1 / O으로 대치\n",
    "\n",
    "model_result_df['result'].astype('float')\n",
    "\n",
    "\n",
    "model_result_df.loc[model_result_df['result']>=0.5, 'pred']='Y'\n",
    "model_result_df.loc[model_result_df['result']<0.5, 'pred']='N'\n",
    "\n",
    "model_result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID값만 다시 출력하기 위해 불러옴\n",
    "test_raw_df = pd.read_csv(test_file_dir)\n",
    "\n",
    "result = pd.concat([test_raw_df['id'],pd.Series(model_result_df['result'])], axis=1)\n",
    "\n",
    "result.to_csv(result_file_name, header=False, index=False, encoding='utf-8')\n",
    "print(\"결과 예측 파일이 출력되었습니다. : \", result_file_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 일반 모델 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if randomforest_model == True:\n",
    "#     result_model = rf_model\n",
    "# elif lightgbm_model == True:\n",
    "#     result_model = lgbm_model\n",
    "# elif catboost_model == True:\n",
    "#     result_model = cat_model\n",
    "# elif xgboost_model == True:\n",
    "#     result_model = xgb_model\n",
    "# elif soft_voting_model == True:\n",
    "#     result_model = soft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #결과 출력할 모델 선택\n",
    "# result_pred = result_model.predict(test_df)\n",
    "\n",
    "# #ID값만 다시 출력하기 위해 불러옴\n",
    "# test_raw_df = pd.read_csv(test_file_dir)\n",
    "\n",
    "# result = pd.concat([test_raw_df['id'],pd.Series(result_pred)], axis=1)\n",
    "\n",
    "# result.loc[result[0]==1, 0]='Y'\n",
    "# result.loc[result[0]==0, 0]='N'\n",
    "\n",
    "# result.to_csv(result_file_name, header=False, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
